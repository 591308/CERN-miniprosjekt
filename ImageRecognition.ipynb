{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CERN & Particle physics: Discrimination of positron/photon/pion showers in EM calorimeter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Frame the problem\n",
    "Datasett link: https://zenodo.org/record/573298#.Y-n-mXaZNtZ\n",
    "Given the hdf5 file that have a following structure:\n",
    "energy    Dataset {100000, 1}\n",
    "layer_0   Dataset {100000, 3, 96}\n",
    "layer_1   Dataset {100000, 12, 12}\n",
    "layer_2   Dataset {100000, 12, 6}\n",
    "overflow  Dataset {100000, 3}\n",
    "In practice, each file is a collection of 100,000 calorimeter showers corresponding to the particle specified in the file name (eplus = positrons, gamma = photons, piplus = charged pions).\n",
    "\n",
    "The calorimeter we built is segmented longitudinally into three layer with different depths and granularities. In units of mm, the three layers have the following (eta, phi, z) dimensions:\n",
    "Layer 0: (5, 160, 90) | Layer 1: (40, 40, 347) | Layer 2: (80, 40, 43)\n",
    "\n",
    "In the HDF5 files, the `energy` entry specifies the true energy of the incoming particle in units of GeV. `layer_0`, `layer_1`, and `layer_2` represents the energy deposited in each layer of the calorimeter in an image data format. Given the segmentation of each calorimeter layer, these images have dimensions 3x96 (in layer 0), 12x12 (in layer 1), and 12x6 (in layer 3). The `overflow` contains the amount of energy that was deposited outside of the calorimeter section we are considering.\n",
    "\n",
    "Train classifier based on image recognition. Utilize 3D information.\n",
    "Convert data arrays into suitable image format. Train CNN. Each event consist of three data/image layers - investigate how this can be treated. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Set up enviroment, import data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv3D, MaxPooling3D, Flatten, Dense\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: <KeysViewHDF5 ['energy', 'layer_0', 'layer_1', 'layer_2', 'overflow']>\n",
      "<class 'h5py._hl.dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "# Code below can show waht are different columns existing in one hdf5 file\n",
    "filename = \"eplus.hdf5\"\n",
    "\n",
    "with h5py.File(filename, \"r\") as f:\n",
    "    # Print all root level object names (aka keys) \n",
    "    # these can be group or dataset names \n",
    "    print(\"Keys: %s\" % f.keys())\n",
    "    # get first object name/key; may or may NOT be a group\n",
    "    a_group_key = list(f.keys())[0]\n",
    "\n",
    "    # get the object type for a_group_key: usually group or dataset\n",
    "    print(type(f[a_group_key])) \n",
    "\n",
    "    # If a_group_key is a group name, \n",
    "    # this gets the object names in the group and returns as a list\n",
    "    data = list(f[a_group_key])\n",
    "\n",
    "    # If a_group_key is a dataset name, \n",
    "    # this gets the dataset values and returns as a list\n",
    "    data = list(f[a_group_key])\n",
    "    # preferred methods to get dataset values:\n",
    "    ds_obj = f[a_group_key]      # returns as a h5py dataset object\n",
    "    ds_arr = f[a_group_key][()]  # returns as a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"eplus.hdf5\", \"r\") as f:\n",
    "    energy = f['energy'][:]\n",
    "    layer_0 = f['layer_0'][:]\n",
    "    layer_1 = f['layer_1'][:]\n",
    "    layer_2 = f['layer_2'][:]\n",
    "    overflow = f['overflow'][:]\n",
    "\n",
    "layer_0_resized = np.array([resize(layer_0[i], (12, 12)) for i in range(layer_0.shape[0])])\n",
    "\n",
    "layer_2_resized = np.array([resize(layer_2[i], (12, 12)) for i in range(layer_2.shape[0])])\n",
    "   \n",
    "images = np.concatenate([layer_0_resized[..., np.newaxis],\n",
    "                        layer_1[..., np.newaxis],\n",
    "                        layer_2_resized[..., np.newaxis]], axis=-1)\n",
    "\n",
    "#sample_image = images[0]\n",
    "\n",
    "#fid, ax = plt.subplots(nrows=1, ncols=3,figsize=(15, 5))\n",
    "#for i in range(3):\n",
    "#   ax[i].imshow(sample_image[i])\n",
    "    \n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the image data\n",
    "images = (images - np.mean(images)) / np.std(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training, validation, and test sets\n",
    "train_data = images[:70000]\n",
    "train_labels = energy[:70000]\n",
    "val_data = images[70000:80000]\n",
    "val_labels = energy[70000:80000]\n",
    "test_data = images[80000:]\n",
    "test_labels = energy[80000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the labels to categorical format\n",
    "train_labels = to_categorical(train_labels)\n",
    "val_labels = to_categorical(val_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture consists of the following components:\n",
    "- Input Layer: It takes an image of shape (12, 12, 6) as input\n",
    "- Convolutional Layers: A set of Convolutional Layers are used to extract features from the input image. The convolutional Layer applies filters (weights) to the input image and performs element-wise multiplications and sums to produce an activation map\n",
    "- Max Pooling Layers: After each Convolutional Layer, a Max Pooling Layer is used to reduce the size of the activation map, thus reducing the number of parameters in the model. Max Pooling Layers perform a down-sampling operation by selecting the maximum value from a set of values in a pooling window\n",
    "- Droput Layers: Droput Layers are used to prevent overfitting of the model. It works by randomly setting a portion of the inputs to zero durin each training iteration.\n",
    "- Flatten Layer: The Flatten Layer converts the 3D feature map into a 1D vector, which can be fed into hte fully connected layer\n",
    "- Dense Layer: The Dense Layer is a fully connected layer that performs classification based on the features extracted by the Convolutional Layers\n",
    "- Output Layer: The Output Layer produces the final classification result by applying a sfotmax activation function to the outputs of the Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The last dimension of the inputs to a Dense layer should be defined. Found None. Full input shape received: (None, None)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m model\u001b[39m.\u001b[39madd(MaxPooling3D(pool_size\u001b[39m=\u001b[39m(\u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m)))\n\u001b[0;32m      5\u001b[0m model\u001b[39m.\u001b[39madd(Flatten())\n\u001b[1;32m----> 6\u001b[0m model\u001b[39m.\u001b[39;49madd(Dense(\u001b[39m64\u001b[39;49m, activation\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mrelu\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m      7\u001b[0m model\u001b[39m.\u001b[39madd(Dense(\u001b[39m10\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m      9\u001b[0m \u001b[39m# Compile the model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\vlads\\anaconda3\\envs\\DAT255\\lib\\site-packages\\tensorflow\\python\\trackable\\base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m   result \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    206\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m previous_value  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\vlads\\anaconda3\\envs\\DAT255\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\vlads\\anaconda3\\envs\\DAT255\\lib\\site-packages\\keras\\layers\\core\\dense.py:148\u001b[0m, in \u001b[0;36mDense.build\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    146\u001b[0m last_dim \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mdimension_value(input_shape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[0;32m    147\u001b[0m \u001b[39mif\u001b[39;00m last_dim \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 148\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    149\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe last dimension of the inputs to a Dense layer \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    150\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mshould be defined. Found None. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    151\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFull input shape received: \u001b[39m\u001b[39m{\u001b[39;00minput_shape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    152\u001b[0m     )\n\u001b[0;32m    153\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_spec \u001b[39m=\u001b[39m InputSpec(min_ndim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, axes\u001b[39m=\u001b[39m{\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m: last_dim})\n\u001b[0;32m    154\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_weight(\n\u001b[0;32m    155\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mkernel\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    156\u001b[0m     shape\u001b[39m=\u001b[39m[last_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munits],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    161\u001b[0m     trainable\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    162\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: The last dimension of the inputs to a Dense layer should be defined. Found None. Full input shape received: (None, None)"
     ]
    }
   ],
   "source": [
    "# Define the model architecture\n",
    "model = Sequential()\n",
    "model.add(Conv3D(32, (3, 3, 3), activation='relu', input_shape=(batch_size, 96, 12, 12)))\n",
    "model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(train_data, train_labels, epochs=10, batch_size=32, validation_data=(val_data, val_labels))\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss, test_acc = model.evaluate(test_data, test_labels)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\vlads\\anaconda3\\envs\\DAT255\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\vlads\\anaconda3\\envs\\DAT255\\lib\\site-packages\\keras\\engine\\training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\vlads\\anaconda3\\envs\\DAT255\\lib\\site-packages\\keras\\engine\\training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\vlads\\anaconda3\\envs\\DAT255\\lib\\site-packages\\keras\\engine\\training.py\", line 1023, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\vlads\\anaconda3\\envs\\DAT255\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\vlads\\anaconda3\\envs\\DAT255\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_1\" is incompatible with the layer: expected shape=(None, 12, 12, 12, 12), found shape=(None, 12, 12, 3)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Train the model on the training data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model\u001b[39m.\u001b[39;49mfit(train_data, train_labels, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m(val_data, val_labels))\n\u001b[0;32m      4\u001b[0m \u001b[39m# Evaluate the model on the test data\u001b[39;00m\n\u001b[0;32m      5\u001b[0m test_loss, test_acc \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(test_data, test_labels)\n",
      "File \u001b[1;32mc:\\Users\\vlads\\anaconda3\\envs\\DAT255\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filejkml8gug.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\vlads\\anaconda3\\envs\\DAT255\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\vlads\\anaconda3\\envs\\DAT255\\lib\\site-packages\\keras\\engine\\training.py\", line 1233, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\vlads\\anaconda3\\envs\\DAT255\\lib\\site-packages\\keras\\engine\\training.py\", line 1222, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\vlads\\anaconda3\\envs\\DAT255\\lib\\site-packages\\keras\\engine\\training.py\", line 1023, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\vlads\\anaconda3\\envs\\DAT255\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\vlads\\anaconda3\\envs\\DAT255\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_1\" is incompatible with the layer: expected shape=(None, 12, 12, 12, 12), found shape=(None, 12, 12, 3)\n"
     ]
    }
   ],
   "source": [
    "# Train the model on the training data\n",
    "model.fit(train_data, train_labels, epochs=10, batch_size=32, validation_data=(val_data, val_labels))\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss, test_acc = model.evaluate(test_data, test_labels)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create pngs?\n",
    "Read the hdf5 file and store each of the image layers as pngs subfolders named by particle name\n",
    "get kind of particle\n",
    "dir to store the calorimeter\n",
    "load and store the images from hdf5\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create RGB images\n",
    "Create form three channels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DAT255",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d664d9bb41b9d3fab976584bc1e1a8560719b55467dc411ee4a441179259a613"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
